---
title: "The Tree of Knowledge"
author: "Kaushik Mohan"
output: github_document
---

## Directory Structure
- `Code/Data` contains edgelists and node details of the scraped network used in this project
- `Paper` includes the Muchnik, et.al. paper used as the primary reference for this project
- `Presentations` contain the project proposal and final presentation for the course

# Introduction

## Objective  
- To extract hierarchical structure of articles for a field (say, Mathematics, Physics..)

## Why?  
- Individual learning is still hierarchical
- Can be used to create structured curriculum content with just resources available on the web
- Identify gaps in information on the web
   
## Overview 
This project is an attempt to use (and improve) methods described in the paper by Muchnik, et.al. for extracting hierarchies from a network. The paper has been made available for reference in the respective folder within the repo. The end goal is to be able to use these techniques to create a knowledge tree from the Wikipedia EN article network which could be used to create a more structured online-reference/learning platform akin to a traditional Encyclopedia.    

For the purpose of this project though, we use a smaller and more specific network of articles from HyperPhysics, a HTML Textbook (http://hyperphysics.phy-astr.gsu.edu/hbase/hframe.html). This textbook also has a more complete hyperlink structure compared to Wikipedia which should give us better results. Based on this network, we should be able to arrive at a tree structure which denotes the evolution of Physics literature and traditional structure in Physics instruction. The HyperPhysics page provides hierarchies based on content and some semantics which could be used as a reference to validate the results of the analysis.

As a work in progress at the end, we look at developing a better algorithm for extracting hierarchies using the idea of community detection to create each level of the hierarchy iteratively. The intution/hypothesis behind formulating such an approach is two-fold. Firstly, the previous methodologies identify hierarchies locally across all nodes to generate the full structure. Secondly, the cutoffs based on which hierarchical relationships are determined arbitrarily and tuned through validation. The proposed methodology addresses these issues by using eigenvector based community detection to create the hierarchy in a top-down approach. By first identifying a community of nodes (topics, in this context) and their hierarchical relationships, we then dive deeper into each topic to determine the hierarchies within. By iteratively going through this process, we hope to have a better and more intuitive hierarchical structure developing from the underlying network. This approach might perhaps more appilicable in this setting as the hypothesis is tied to the context.   

## Goals

To summarize, the goals of this project are

1. Use the methodologies defined in the paper to extract the hierarchical structure of HyperPhysics content

2. Develop a commuity based iterative methodology for hierarchy extraction in a knowledge network

3. Compare the results from the different methodologies and validate the results by surveying members in the Physics community

***

## Part 1: Scraping for Data

The first part of the project involves scraping through the HyperPhysics domain to create a network with pages as nodes and hyperlink between pages as edges. The webpage used here is HyperPhysics, a Physics and Math HTML textbook developed by Carl R. nave of Georgia State University. We use this because it's comprehensive coverage of most of physics and the linking structure between content pages. It also has a map of the content in some form which could provide validation to the extraction of trees from the link structure.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(rvest)
library(xml2)
library(stringr)
library(stringi)
```

The url is http://hyperphysics.phy-astr.gsu.edu/hbase/hframe.html. We use the package 'rvest' which is similar to 'beautifulsoup' in Python used for web-scraping. 

```{r}
## We start with the first article in the Index, acceleration as the index page itself is a hframe and we are unable to parse it for links. 

url <- "http://hyperphysics.phy-astr.gsu.edu/hbase/acca.html"
webpage <- read_html(url)
nodes <- html_nodes(webpage,'a')
titles <- html_text(nodes)
links <- tolower(html_attr(nodes,"href"))
headings <- html_nodes(webpage,'h1')
h_regex <- regexpr(">(.*)<",as.character(headings[1]))
heading <- substr(as.character(headings[1]),(h_regex[1]+1),h_regex[1]+attr(h_regex,"match.length")-2)
heading
links
titles

```

We can see all the links and page titles extracted from the HTML page. Next, we need to traverse through these URLs to find links in those iteratively. We keep track of the URLs visted and also include some URLs that we do not wish to look into as they do not have topic related content. In the below chunk, we write the function to extract all the links from a page while simultaneously cleaning the data for further use. We exclude javascript links, multimedia links and also stop the parsing from entering into an infinite loop when it reaches an index directory (http://hyperphysics.phy-astr.gsu.edu/hbase/Kinetic/) where sorting by Name, Size etc. create a slightly different URL. We also exclude URLs under the class/ subdomain as these don't contain Physics content but are structured courses that have been created within the hyperphysics domain. We account for many such cases and define a function to scrape for hyperlinks and simultaneously clean it for consumption. 

```{r, warning=FALSE, message=FALSE,echo=FALSE}
url_prefix <- "http://hyperphysics.phy-astr.gsu.edu/hbase/"

## Defining a function to get all the hyperlinks in a given webpage
get_links <- function(url,prefix=url_prefix,heading){
  
  full_url <- paste0(prefix,url)
  webpage <- read_html(full_url)
  nodes <- html_nodes(webpage,'a')
  titles <- stri_encode(html_text(nodes), "", "UTF-8")
  links <- html_attr(nodes,"href")
  headings <- html_nodes(webpage,'h1')
  h_regex <- regexpr(">(.*)<",as.character(headings[1]))
  heading <- substr(as.character(headings[1]),(h_regex[1]+1),h_regex[1]+attr(h_regex,"match.length")-2)

  ## Removing any javascript or external (starting with http:// or https://) links 
  titles <- titles[!grepl("^javascript|Javascript|http://|https://|www.|mailto:",links)]
  links <- links[!grepl("^javascript|Javascript|http://|https://|www.|mailto:",links)]
  
  ## Removing multimedia links (.gif/.mp4/.jpg/.png/.mov)
  titles <- titles[!grepl(".gif|.jpg|.png|.mp4|.mov|.jpeg$",links)]
  links <- links[!grepl(".gif|.jpg|.png|.mp4|.mov|.jpeg$",links)]
  
  ## Removing any links which refer to index directory sorting
  titles <- titles[!grepl("[[:punct:]]C[[:punct:]][A-Z][[:punct:]]O[[:punct:]][A-Z]",links)]
  links <- links[!grepl("[[:punct:]]C[[:punct:]][A-Z][[:punct:]]O[[:punct:]][A-Z]",links)]
  
  ## Removing certain URLs to exclude within page references (of the form #xxxD)
  links <- sub("\\#.*","\\",links)
  
  ## Removing the Index and Main page URLs along with any null or missing cases
  excluded_urls <- c("",NA)
  titles <- titles[!links %in% excluded_urls]
  links <- links[!links %in% excluded_urls]
  
  ## If the parent url is of the form xx/yy/zz.html, we need to prefix yy/ to results of the form  xx.html
  if(grepl("^.*/",url)){
    string_to_add <- paste0(sub("\\/.*html","\\",url),"/")
    links[!grepl("^../",links)] <- paste0(string_to_add,links[!grepl("^../",links)])
  }
  
  ## Removing any links to sub-domain class as they don;t have physics content
  titles <- titles[!grepl("class/",links)]
  links <- links[!grepl("class/",links)]

  ## Trimming URLs of the form ../xxx.html and removing extra /s and spaces
  links <- sub("^../","\\",links)
  links <- gsub("//|///","/",links)
  links <- gsub("\r","",links)
  
  ## Fixing broken URLs (manual fixes during initial testing)
  links[grepl("^mechanics/vel.html$",links)] <- "vel.html"
  links[grepl("^mechanics/frict.html$",links)] <- "frict.html"
  links[grepl("^forces/particles/quark.html$",links)] <- "particles/quark.html"
  links[grepl("^magnetic/ferro.html$",links)] <- "solids/ferro.html"
  links[grepl("^nuclear/hframe.html$",links)] <- "hframe.html"
  links[grepl("^mechanics/hframe.html$",links)] <- "hframe.html"
  links[grepl("^mechanics/hph.html$",links)] <- "hph.html"
  links[grepl("^thermo/therm/entropcon.html$",links)] <- "thermo/entropcon.html"
  links[grepl("^astro/particles/hadron.html$",links)] <- "particles/hadron.html"
  links[grepl("^astro/grav.html$",links)] <- "grav.html"

  
  ## Removing the Index and Main page URLs along with any null or missing cases
  excluded_urls <- c("hframe.html","hph.html")
  titles <- titles[!links %in% excluded_urls]
  links <- links[!links %in% excluded_urls]
  links <- tolower(links)
  
  ## trimming leading and trailing white spaces and removing duplicates 
  unique_links <- trimws(links[!duplicated(links)],which="both")
  unique_titles <- trimws(titles[!duplicated(links)],which="both") 
  return(list(main_page=url,main_page_title=heading,page_links=unique_links,page_titles=unique_titles))
}

l <- get_links("acca.html",heading="Acceleration")
l

```

We note that the results now are lot cleaner compared to the raw output from before. Now that we have written a function to extract urls and titles, we will create a data structure for storing this information for all the pages we visit. We also define functions to add new pages and edges to the respective data frames.

```{r, echo=FALSE}
## Storing the page URL and title in data frame page_details
page_details <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
colnames(page_details) <- c("url","title")

## Initializing a data frame to store the edge list which would be the hyperlinks
edge_list <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
colnames(edge_list) <- c("from_url","to_url")
```

```{r, echo=FALSE}
## Function to add new page details
add_page_details <- function(l,df){
  existing_urls <- df$url
  existing_titles <- df$title
  new_urls <- c(l$main_page,l$page_links)
  new_titles <- c(l$main_page_title,l$page_titles)
  ## Checking if Title is as per page and not the hyperlink text
  if(!(df[df$url == l$main_page,2] == l$main_page_title)){
    df[df$url == l$main_page,2] <- l$main_page_title
  }
  ## Removing duplicates before appending
  new_titles <- new_titles[!new_urls %in% existing_urls]
  new_urls <- new_urls[!new_urls %in% existing_urls]
  temp_df <- data.frame(url=new_urls,title=new_titles)
  return(rbind(df,temp_df))
}

## Function to get new edge information
get_edges <- function(l){
  if(length(l$page_links) > 0){
    return(data.frame(from_url=l$main_page,to_url=l$page_links))
  }else{
    return(NULL)
  }
}

```

Using these helper functions, we go over all the URLs and store the network structure. 

```{r, warning=FALSE, message=FALSE, echo=FALSE}
## Creating a list to store all the URLs vsiited
visited_urls <- NA
error_urls <- NA
unvisited_urls <- c("acca.html") ## starting with the first URL
page_details <- rbind(page_details,c("acca.html","Acceleration"))
page_details <- page_details[-1,]

####################
# Function to go through a starting set of URLs and iteratively visit all linked URLs till no none remain unvisited
####################

scrape_urls <- function(urls_to_visit,visited_urls, page_details, edge_list,error_urls){
  counter <- 0
  st <- proc.time()
  while(length(urls_to_visit) > 0){
    tryCatch(
      {
        l <- get_links(urls_to_visit[1],heading=page_details$title[which(page_details$url == urls_to_visit[1])])
        page_details <- add_page_details(l,df = page_details )
        edge_list <- rbind(edge_list,get_edges(l))
      },
      error=function(cond){
        # print(unvisited_urls[1])
        error_urls <<- c(error_urls,urls_to_visit[1])
      },
      finally={
        visited_urls <- c(visited_urls,urls_to_visit[1])
        urls_to_visit <- page_details$url[!page_details$url %in% visited_urls]
        counter <- counter + 1   
      }
    )
  }
  error_urls <- error_urls[!is.na(error_urls)]
  print(proc.time() - st)
  return(list(pd=page_details,el=edge_list,visited_urls=visited_urls,unvisited_urls=urls_to_visit,errors=error_urls))
}

## Running the scraping function

scrape <- scrape_urls(urls_to_visit=unvisited_urls,visited_urls=visited_urls, page_details=page_details, edge_list=edge_list,error_urls = error_urls)

unvisited_urls <- scrape$unvisited_urls
visited_urls <- scrape$visited_urls
page_details <- scrape$pd
edge_list <- scrape$el
error_urls <- scrape$errors
```

### Error handling

One of the major challenges in scraping the web for the network data was with handling the different formats of URLs and account for broken ones. At the end of the scraping, we find that there are 295 broken URLs. The next step is to try and fix these before scraping through them again. Through some manual inspection, we try and find patterns within these in order to fix them in batches rather than individually. We are able to identify that most of the cases involve a missing or incorrect parent domain. By taking care of these cases, we reduce the number down to less than 20 cases. 


```{r, warning=FALSE,message=FALSE, echo=FALSE}
length(error_urls)

#######################
## Fixing Broken URLs
#######################

## Removing the links which are broken because of the index directory structure
error_urls <- error_urls[!(grepl("^kinetic/imgkin",error_urls))]
error_urls <- error_urls[!(grepl("^kinetic/kinpic",error_urls))]
error_urls <- error_urls[!(grepl("kinetic/hbase/",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/imgqua",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/modpic",error_urls))]
error_urls <- error_urls[!(grepl("quantum/hbsae/",error_urls))]
error_urls <- error_urls[!(grepl("^thermo/heatpic",error_urls))]
error_urls <- error_urls[!(grepl("^thermo/imgheat",error_urls))]
error_urls <- error_urls[!(grepl("thermo/hbase/",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/imgqua/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/modpic/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^kinetic/[[:punct:]]",error_urls))]

## Creating a data frame to store these URLs and their fixes
fixed_error_urls <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
unvisited_urls <- ""

## Looping through error URLs and identifying those of the form described above
for(i in seq_along(error_urls)){
  fixed_error_urls[i,1] <- error_urls[i]
  tryCatch(
    {
      fixed_error_urls[i,2] <- substring(error_urls[i],regexpr("/",error_urls[i])[1]+1)
      temp_url <- paste0(url_prefix,fixed_error_urls[i,2])
      if(!(fixed_error_urls[i,2] %in% edge_list$from_url)){
        temp_page <- read_html(temp_url)  
      }
      ## Replace URL in Page Details
      page_details$url[which(page_details$url %in% error_urls[i])] <- fixed_error_urls[i,2]
      ## Replace URL in Edge Details
      edge_list$from_url[which(edge_list$from_url %in% error_urls[i])] <- fixed_error_urls[i,2]
      edge_list$to_url[which(edge_list$to_url %in% error_urls[i])] <- fixed_error_urls[i,2]
      ## Add to Unvisited URLs if not visited
      if(!fixed_error_urls[i,2] %in% edge_list$from_url){
        unvisited_urls <- c(unvisited_urls,fixed_error_urls[i,2])
      }
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    }
  )
}

## Function to fix URLs in page details and edge list to the correct URL and adding to list of pages to visit if unvisited

fix_error_urls <- function(fixed_error_urls, page_details, edge_list,unvisited_urls){
  error_url_fixes <- fixed_error_urls[!is.na(fixed_error_urls$X2),]
  for(i in seq_along(error_url_fixes)){
    err_url <- error_url_fixes$X1[i]
    fixed_url <- error_url_fixes$X2[i]
    page_details$url[which(page_details$url == err_url)] <- fixed_url
    edge_list$from_url[which(edge_list$from_url == err_url)] <- fixed_url  
    edge_list$to_url[which(edge_list$to_url == err_url)] <- fixed_url  
    if(!fixed_url %in% edge_list$from_url){
          unvisited_urls <- c(unvisited_urls,fixed_url)
        }
  }
  fixed_error_urls <- fixed_error_urls[is.na(fixed_error_urls$X2),]
  return(list(errors=fixed_error_urls,pd=page_details,el=edge_list,unvisited=unvisited_urls))
}

fixes <- fix_error_urls(fixed_error_urls = fixed_error_urls, page_details = page_details, edge_list = edge_list, unvisited_urls = unvisited_urls)
unvisited_urls <- fixes$unvisited
page_details <- fixes$pd
edge_list <- fixes$el
fixed_error_urls <- fixes$errors

## Check if these URls are infact broken (some seem to work on manual inspection)
for(i in seq_along(fixed_error_urls$X1)){
  tryCatch(
    {
      temp_url <- paste0(url_prefix,gsub("//|///","/",fixed_error_urls[i,1]))
      temp_page <- read_html(temp_url)   
      fixed_error_urls[i,2] <- gsub("//|///","/",fixed_error_urls[i,1])
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    }
  )
}

fixes <- fix_error_urls(fixed_error_urls = fixed_error_urls, page_details = page_details, edge_list = edge_list, unvisited_urls = unvisited_urls)
unvisited_urls <- fixes$unvisited
page_details <- fixes$pd
edge_list <- fixes$el
fixed_error_urls <- fixes$errors
# dim(fixed_error_urls)[1]

```

```{r, warning=FALSE,message=FALSE, echo=FALSE}

## Adding parent domain to URL to check if it works
for(i in seq_along(fixed_error_urls$X1)){
  tryCatch(
    {  
      parent_url <- edge_list$from_url[edge_list$to_url == fixed_error_urls[i,1]][2]
      parent_domain <- strsplit(parent_url,"/")[[1]][max(1,length(strsplit(parent_url,"/")[[1]])-1)]
      fixed_url <- paste0(parent_domain,"/",fixed_error_urls$X1[i])
      temp_url <- paste0(url_prefix,fixed_url)
      temp_page <- read_html(temp_url)   
      fixed_error_urls[i,2] <- fixed_url
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    }
  )
}

## Other manual fixes
fixed_error_urls$X2[grep("nuclear/gphysref.html",fixed_error_urls$X1)] <-"geophys/gphysref.html"
fixed_error_urls$X2[grep("phyopt/optref.html",fixed_error_urls$X1)] <-"optics/optref.html"
fixed_error_urls$X2[grep("astref.html",fixed_error_urls$X1)] <-"astro/astref.html"
fixed_error_urls$X2[grep("relcon.html",fixed_error_urls$X1)] <-"relativ/relcon.html"
fixed_error_urls$X2[grep("music/..sound/timbre.html",fixed_error_urls$X1)] <-"sound/timbre.html"

fixes <- fix_error_urls(fixed_error_urls = fixed_error_urls, page_details = page_details, edge_list = edge_list, unvisited_urls = unvisited_urls)
unvisited_urls <- fixes$unvisited
page_details <- fixes$pd
edge_list <- fixes$el
fixed_error_urls <- fixes$errors
# dim(fixed_error_urls)[1]

head(fixed_error_urls[,1],20)
## Ignoring 20 broken URLs for now
```

We ignore the 20 remaining cases, as it was difficult to manually identify the correct URLs for these. Next, we visit the unvisited fixed URLs and add the information to the dataset. 

```{r, warning=FALSE,message=FALSE, echo=FALSE}

## Re-running the scraping function with the fixed URLs unvisited previously
new_error_urls <- ""
scrape <- scrape_urls(urls_to_visit=unvisited_urls,visited_urls=visited_urls, page_details=page_details, edge_list=edge_list,error_urls = new_error_urls)

unvisited_urls <- scrape$unvisited_urls
visited_urls <- scrape$visited_urls
page_details <- scrape$pd
edge_list <- scrape$el
new_error_urls <- scrape$errors

```

Finally, we check the dataset once more to remove self-edges and any duplicates.

```{r, warning=FALSE, message=FALSE}
## Removing self edges from edge_list and 1st row with an NA
edge_list <- edge_list[-1,]
edge_list <- edge_list[-which(edge_list$from_url == edge_list$to_url),]

## Checking for and removing duplicate entries in page_details and edge_list
edge_list <- edge_list[!duplicated(edge_list), ]
page_details <- page_details[!duplicated(page_details),] ## URL and Title both match
page_details <- page_details[!duplicated(page_details$url),] ## Only URL duplicated
```

We store these final Edge List and Page Details data frames as CSVs as a backup, for easier access in the following sections and to work with a clean environment.

```{r, warning=FALSE, message=FALSE}
setwd("Code/Data")
write.csv(page_details,file="page_details.csv",row.names = FALSE)
write.csv(edge_list,file="edge_list.csv",row.names = FALSE)
```

We are finally left with 21,057 edges and 2931 nodes. We use this network data for further analysis in the following sections.

***

## Part 2: Network Stats

In this section, we use this data from the scraping exercise to create a network object and perform some basic network analysis. We look at presenting a high-level overview of the network through some of it's node level and network level statistics in order gain an understanding of the underlying structure. 

```{r, echo=FALSE,warning=FALSE,message=FALSE}
rm(list = ls())
library(igraph)
library(sna)
library(network)
library(statnet)
library(intergraph)
library(ggraph)
```

Importing the saved datasets with edge list and nodal attributes. 

```{r, echo=FALSE, warning=FALSE}
## Importing Data Sets
edge_list <- read.csv("Code/Data/edge_list.csv",stringsAsFactors = FALSE)
page_details <- read.csv("Code/Data/page_details.csv",stringsAsFactors = FALSE)
```

Creating the network objects and setting the vertex attributes. When the network is created, it numbers the nodes alphabetically based on the vertex name (in this case URL). We get these and store them for future reference. 

```{r, echo=FALSE}
## 
page_details[grep("HyperMath",page_details$title),2] <- "HyperMath"
page_details[grep("hph.html",page_details$url),2] <- "HyperPhysics"

## Creating Network object
hyperphysics_network <- network(as.matrix(edge_list),matrix.type="edgelist",directed = TRUE)

node_names <- get.vertex.attribute(hyperphysics_network,"vertex.names") 
set.vertex.attribute(hyperphysics_network,"Titles",page_details$title[order(page_details$url)])

## IGraph object
hyperphysics_graph <- asIgraph(hyperphysics_network)

## Storing the Vertex URLs and Titles in the order they appear in the graph object
vertex_urls <- vertex_attr(hyperphysics_graph,"vertex.names")
vertex_titles <- vertex_attr(hyperphysics_graph,"Titles") 

## Some additional clean up of the data (remove trailing spaces and /n)
vertex_titles <- gsub("\n","",vertex_titles)
vertex_titles <- trimws(vertex_titles,which = "both")
V(hyperphysics_graph)$Titles <- vertex_titles

```

#### Sample of the network

```{r}
head(page_details,5)
head(edge_list,5)
```

##### Plot

```{r, echo=FALSE}
set.seed(5)
sample_nodes <- sample(V(hyperphysics_graph),750,replace = FALSE)
sub_hyperphysics_graph <- induced_subgraph(hyperphysics_graph,sample_nodes)
plot(sub_hyperphysics_graph,edge.arrow.size=.3,vertex.size=3,vertex.label=NA)
```

From the plot of an induced sub-graph of 750 nodes, we can already see some clusters closely interconnected within in the network. 

### Nodes, Edges and Density

```{r, echo=FALSE}
paste("No. of Nodes: ",dim(page_details)[1])
paste("No. of Edges: ",dim(edge_list)[1])
network_density <- dim(edge_list)[1]/(dim(page_details)[1]*(dim(page_details)[1]-1))
paste("Density: ",round(network_density,5))

```

### Degree Distribution
The first network statistics we look into furhter is the degree distribution. We want to verify if the distribution follows a power law as with several similar networks extracted from the web.

```{r,echo=FALSE}
## Degree Distribution
in_degree <- igraph::degree(hyperphysics_graph,mode="in")
out_degree <- igraph::degree(hyperphysics_graph,mode="out")

## Avg. Degrees
paste("Mean In-Degree: ",round(mean(in_degree),4))
paste("Mean Out-Degree: ",round(mean(out_degree),4))
```

It is a bit strange and interesting that the Avg. In and Out Degrees match perfectly. We then plot the degree distribution in the log-log scale to see if it is linear denoting a scale-free distribution.

```{r, echo=FALSE}
## Plots of Degree Distribution
par(mfrow=c(1,2))
plot(log(as.numeric(row.names(table(in_degree)))),log(table(in_degree)),main="Log-log plot of In-Degree Dist.",xlab="log(In-Degree)",ylab="log(frequency)",type="p",pch=1)

plot(log(as.numeric(row.names(table(out_degree)[-1]))),log(table(out_degree)[-1]),main="Log-log plot of Out-Degree Dist.",xlab="log(Out-Degree)",ylab="log(frequency)",type="p",pch=1)

```

We note that the degree distributions is almost perfectly linear in the log-log scale signifying preferential attachment in terms of the network structure. The out-degree distribution though starts off with a lower frequency of nodes with fewer degress. This perhaps indicates the nature of the textbook where most of the pages are linked with atleast a few other pages and very few pages have no links outward. 

```{r, echo=FALSE}
in_deg_est <- lm(log(table(in_degree))~log(as.numeric(row.names(table(in_degree)))))
out_deg_est <- lm(log(table(out_degree)[-1])~log(as.numeric(row.names(table(out_degree)[-1]))))

in_deg_est$coefficients[2]
out_deg_est$coefficients[2]
```

The estimated co-efficient or the power in the power-law form for the degree distributions are -1.365 and -1.552 for the In and Out degrees respectively. That is the in and out degrees roughly follow the following distributions. 

![Degree Distribution](images/degrees.png)


### Geodesic Distance Distribution

Next, we look at the distrbution of the Geodesic distance between nodes. This measure gives us a sense of how close the nodes are from each other and along side a scale-free degree distribution, a short average geodesic distance (or network diameter) would indicate a small-world nature of the network as defined by Watts and Strogatz (1998).

```{r, echo=FALSE}
## Geodesic Distances

geodesic_distances <- geodist(hyperphysics_network,count.paths = FALSE)
avg_path_length <- average.path.length(hyperphysics_graph)
dist_path_length <- path.length.hist(hyperphysics_graph)
paste("Average Geodesic Distance: ",round(avg_path_length,3))
options(scipen=3)
plot(c(1:length(dist_path_length$res)),dist_path_length$res,main="Distribution of Geodesic Distances",xlab = "Geodesic Path length",ylab="Count",type='o',pch=1)

```

We do observe that the Network Diameter small at ~5.86 and this is re-inforced by the above plot of the geodesic distance distribution. 

### Centralization

The key network and nodal measure for our analysis is the Centrality and Centralization measures.The overall network centralization is 0.236. This gives us a sense of the centralization of the network relative to a star configuration which is maximally centrlized. A value of 0.236 implies that the network isn't highly centralized but at the same time the structure isn't purely random either with some nodes playing a central role. 

```{r, echo=FALSE}
## Centrality Measures
betweenness_centrality <- centr_betw(hyperphysics_graph)
paste("Network Centralization: ",round(betweenness_centrality$centralization,3))
page_rank_centrality <- page_rank(hyperphysics_graph,algo="prpack")
```

***

## Part 3: Extracting hierarchies

In this section, we create the functions to extract hierarchies from the network and use these on the HyperPhysics article network. Four different hierarchy extraction methods are explored, namely

1. Betweenness Centrality based  
2. Page-Rank based  
3. Attraction Basin  
4. Eigenvector based

The specific formulae and methodology are presented in following sub-sections where we explore each of these methodologies in detail. The first three methods listed above are as defined in the Muchnik, et.al. paper. The last methodology is an independent exploration of a different way to extract hierarchy which takes into account the clusters of topics in the network and extract the hierarchy iteratively starting from the highest-level allthe way down to the leaf nodes.  


```{r, echo=FALSE}

## We create the adjacency matrix and an undirected edgelist which is needed for hierarchy extraction

hyperphysics_adjacency <- as_adj(hyperphysics_graph,type = "both")
hyperphysics_edgelist <- as_edgelist(hyperphysics_graph)
undirected_edgelist <- data.frame(rbind(hyperphysics_edgelist,cbind(hyperphysics_edgelist[,2],hyperphysics_edgelist[,1])))
undirected_edgelist <- undirected_edgelist[!duplicated(undirected_edgelist), ]

```

### Overview
To understand the core idea behind extracting hierarchies from a network, we look at a regular tree to start with. From the below sample tree marked by it's corresponding betweenness centralities, we can make the following observations:
    
- Greater centrality corresponds to a higher level in the hierarchy
- Nodes at the same level of hierarchy have similar centrality
- Large difference in centrality for nodes unconnected with by an edge in the hierarchy

```{r, echo=FALSE}
g <- make_tree(40,children=3,mode="out")
bw_g <- centr_betw(g,directed=FALSE)$res
V(g)$bw <- ifelse(bw_g > 0,bw_g,"")
plot(g, vertex.size=8,vertex.label=V(g)$bw ,vertex.label.cex=0.8,vertex.label.dist=2,edge.arrow.size=.3)
```

### 

These three fundamental ideas form the basis of hierarchy extraction in each of the following methods. The overall process of hierarchy extraction can be summarized as the following set of steps

1. Define a suitable hierarchy score for each node:
A hierarchy score for a node is a measure such as it's betweenness centrality which can be used to compare two nodes to determine the hierarchical relation between them. For a regular node such as the above example, we can simply use the betweenness centrality as the graph is regular. But in the case of real-world networks such as the one we are dealing with, every node need not have the same degree and hence we look at nodal measures which are scaled by their respective degrees. 

2. Compare scores for 2 neighboring nodes (in the underlying undirected network):
We then compare the scores of two nodes in the underlying network and determine if one is higher in the hierarchy if the ratio of the scores is between a lower and an upper threshold and it has a higher score. This does not depend on the direction of the edge between the two nodes in the underlying network. 

3. Define the right cutoffs:
The choice of cutoffs affects the hierarchical network output. A high upper cutoff would create hierarchical relations between two similar nodes at the same level in the hierarchy such as two disjoint topics in physics such as Kinetics and Optics, say. A low or no lower cutoff would include a hierarchical relation between two nodes connected in the underlying network but not necessarily directly related in a hierarchical sense. It may leed to creation of triangles in the hierarchy with a node being connected to several nodes at higher levels in the hierarchy. 

The paper performs the analysis on the Wikipedia article network where it's corresponding Wikipedia Category network was used to validate the results of the methodologies. This allowed the authors to optimize the cutoffs based on correct classification of the hierarchical relations with respect to the category network. In our case, we do not have a hierarchical network to validate against and hence try out different levels of cutoff. As a aprt of continuing work on this, I hope to survey educators in the Physics community to help classify the estimated hierarchical relations to validate the results. 

Before going into the methodologies we define some helper functions which help create a hierarchical graph from the respective scores and also functions for plotting purposes. 

```{r, echo=FALSE}
get_hierarchy_graph <- function(h_scores,edgelist,upper,lower){
  
  ## Calculating Score ratio for all edges with error handling for nodes with a 0 score
  score_ratio <- h_scores[edgelist[,1]]/ifelse(h_scores[edgelist[,2]]==0,0.00001,h_scores[edgelist[,2]])
  
  ## Creating a matrix with edges, scores for each node and the ratio of scores
  hierarchy_matrix <- cbind(edgelist,h_scores[edgelist[,1]], h_scores[edgelist[,2]], score_ratio)
  colnames(hierarchy_matrix) <- c("V1","V2","Score_V1","Score_V2","Score_Ratio")
  
  ## Eliminating rows where score ratio is not between the cutoffs
  hierarchy_matrix <- hierarchy_matrix[hierarchy_matrix$Score_Ratio > lower & hierarchy_matrix$Score_Ratio <= upper,]
  
  ## Determining the higher and lower nodes based on their individual scores
  hierarchy_matrix <- cbind(hierarchy_matrix,ifelse(hierarchy_matrix[,3] > hierarchy_matrix[,4],hierarchy_matrix[,1],hierarchy_matrix[,2]))
  hierarchy_matrix <- cbind(hierarchy_matrix,ifelse(hierarchy_matrix[,3] > hierarchy_matrix[,4],hierarchy_matrix[,2],hierarchy_matrix[,1]))
  colnames(hierarchy_matrix) <- c("V1","V2","Score_V1","Score_V2","Score_Ratio","Higher_Node","Lower_Node")

  ## The Last two columns are the directional edges in the hierarchy 
  hierarchy_edgelist <- as.matrix(hierarchy_matrix[,c(6,7)])
  hierarchy_network <- network(hierarchy_edgelist,matrix.type="edgelist",directed = TRUE)
  hierarchy_graph <- asIgraph(hierarchy_network)
  return(hierarchy_graph)
}

```

We also define a function to sample a graph from the hierarchy where, given a staring node, we sample $n$ nodes one level lower and continue on till the given number of levels. We also define two helper functions to wrap the titles of the nodes and create a plotting function for a cleaner visualization.  

```{r,echo=FALSE}
## Recursive Depth-First search for the sub-tree given the root

get_children <- function(st,el,n=5){
  all_children <- matrix(el[el[,1]==st,],ncol=2,byrow=FALSE)
  sampled_children <- all_children[all_children[,2] %in% sample(all_children[,2],min(n,length(all_children[,2])),replace=FALSE),]
  return(matrix(sampled_children,ncol=2,byrow = FALSE))
}

get_sub_tree <- function(st,el,level=1,cur_level=1,sub_el=matrix(NA,ncol=2),n=5){
  if(cur_level > level){
    tmp_el <- matrix(NA,ncol=2)
    return(tmp_el)
  }else{
    tmp_el <- get_children(st,el,n)
    for(i in seq_along(tmp_el[,2])){
      if(!tmp_el[i,2] %in% sub_el[,1]){
        tmp_el <- rbind(tmp_el,get_sub_tree(tmp_el[i,2],el,level,(cur_level+1),tmp_el,n=n))  
      }
    }
  }
  tmp_el <- tmp_el[!duplicated(tmp_el),]
  tmp_el <- tmp_el[!is.na(tmp_el[,1]),]
  return(tmp_el)
}

wrap_strings <- function(vector_of_strings,width){
  as.character(sapply(vector_of_strings, FUN=function(x){
                        paste(strwrap(x, width=width), collapse="\n")
                        }))
}


## Function to plot a tree using ggraph
tree_plot_func <- function(graph,sz=2,title) {

  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel= TRUE,size=sz) +
    labs(title = title) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 10))
  
  print(plot)
}


```


### Betweenness Centrality based hierarchy extraction

As the name suggests, in the method we use a simple betweenness centrality based hierarchy score to determine hierarchical relation between the nodes. As discussed above, in order to scale the score to account for the non-regular network structure, we define a hierarchy score as below:

![Betweenness Centrality Score](images/betweenness_centrality.png)

```{r, warning=FALSE, message=FALSE, echo=FALSE}
betweenness_centrality <- centr_betw(hyperphysics_graph,directed=TRUE)
bw_hierarchy_score <- betweenness_centrality$res/(sqrt((1+in_degree)*(1+out_degree)))

upper_cutoff <- 0.8
lower_cutoff <- 0.2

bw_hierarchy_graph <- get_hierarchy_graph(bw_hierarchy_score,undirected_edgelist,upper_cutoff,lower_cutoff)
bw_hierarchy_edgelist <- as_edgelist(bw_hierarchy_graph)

bw_hierarchy_graph_centrality <- centr_betw(bw_hierarchy_graph,directed=FALSE)
max_centrality_node <- which(bw_hierarchy_graph_centrality$res == max(bw_hierarchy_graph_centrality$res),arr.ind = TRUE)
```



### Page-Rank based hierarchy extraction

The idea behind page-rank based hierarchy scoring is similar to betweenness based approach. Page-Rank algorithm gives us a centrality measure for a node that is weighted by the centrality measures of it's neighbours. As this measure is already weighted, we don't need to scale it by the degree of the node. Page-Rank centrality is defined as below:

![Page-Rank Centrality](images/page_rank_centrality.png)

$$PR(u) = \sum_{v\in B_u}\frac{PR(v)}{L(v)} $$

$PR(u)$: Page-Rank for node $u$  
$B_u$: The set containing all nodes linking to node $u$  
$L(v)$: No. of edges from node $v$

```{r, warning=FALSE, message=FALSE,echo=FALSE}
page_rank_centrality <- page_rank(hyperphysics_graph,algo="prpack",directed=TRUE)
pr_hierarchy_score <- page_rank_centrality$vector

upper_cutoff <- 0.8
lower_cutoff <- 0.2

pr_hierarchy_graph <- get_hierarchy_graph(pr_hierarchy_score,undirected_edgelist,upper_cutoff,lower_cutoff)
pr_hierarchy_edgelist <- as_edgelist(pr_hierarchy_graph)
V(pr_hierarchy_graph)$titles <- strtrim(wrap_strings(vertex_titles[V(pr_hierarchy_graph)],10),30)
```


### Attraction Basin based hierarchy extraction

This is a method developed by the authors of the paper. The purpose behind it was to create a measure that can be estimated locally and does not nequire for the entire network to be analysed. For example, to calculate betweenness centrality for a node, all the shortest paths between all the nodes have to be calculated. Similarly, page-rank centrality being iterative, has to be estimated for all the nodes in the network together. For large networks, such computations can be very expensive and hence there is a need for a local measure of centrality.

The authors define the score as comparision between the weighted fraction of the network from which the node can be reached to the weighted fraction of the network that can be reached from each node. In a sense, the quantity summarizes the "flow"" of information/references through a node. It takes into account local and structural properties of the network using the weighting parameter. A high value of this parameter uses more structural information of the network by using a greater range of influence for a node and low value of the parameter measures the metric locally. The idea of information flow can be better understood with the help of the below image:

![Attraction Basin (source: Muchnik, et.al. (2007))](images/attraction_basin.png)

Given this idea, the hierarchy score is defined as the following:

![Attraction-Basin hierarchy Score](images/attraction_basin_score.png)

```{r, warning=FALSE,message=FALSE, echo=FALSE}
get_ab_hierarchy_score <- function(gdist,alpha,m){
  h_plus <- rep(0,dim(gdist)[1])
  h_minus <- rep(0,dim(gdist)[1])
  
  for(j in c(1:m)){  
    N_plus <- rep(0,dim(gdist)[1])
    N_minus <- rep(0,dim(gdist)[1])
    for(i in c(1:dim(gdist)[1])){
      N_plus[i] <- max(1,sum(gdist[i,] == m))
      N_minus[i] <- sum(gdist[,i] == m)
    }
    N_plus_avg <- mean(N_plus)
    N_minus_avg <- mean(N_minus)
    h_plus <- h_plus + alpha^(-j) * (N_plus / N_plus_avg)
    h_minus <- h_minus + alpha^(-j) * (N_minus / N_minus_avg)
  }
  h_score <- h_minus / h_plus
  return(h_score)
}
```

```{r, echo=FALSE}
ab_hierarchy_score <- get_ab_hierarchy_score(geodesic_distances$gdist,alpha=2,m=3)

upper_cutoff <- 0.8
lower_cutoff <- 0.2

ab_hierarchy_graph <- get_hierarchy_graph(ab_hierarchy_score,undirected_edgelist,upper_cutoff,lower_cutoff)
ab_hierarchy_edgelist <- as_edgelist(ab_hierarchy_graph)

```

***

## Part 4: Results

After defining all the methodologies and computing the overall hierarchy graph, we finally take a look at the results. For ease of visualization, I present a subgraph upto 3 levels in depth and sample of the complete branch structure. In order to compare the results from the 3 methods, we present the sub-graph originating at 3 different topics for each of the methods. The topics explored are

- Conservation Laws
- Biology Concepts
- Quantum Physics

We choose these three topics for multiple reasons. A concept like the Conservation Laws is fundamental to many different fields of physics and would expect it to be high up in the overall hierarchy. By looking at the structure originating from this, we could get a sense of how the hierarchy is determined at the high-level. Next, we choose Biology concepts because it is a tangential topic in the textbook with limited content compared to Physics. By looking at the estimated hierarchies for Biology, we could learn about how each of these methodologies work for a more self contained topic or group of articles. Lastly, we look at Quantum physics because being an advanced and not fully explored as a topic, we would expect it to appear relatively lower in the hierarchy and connected to many leaves. The performance of the algorithm in this domain could tell us how sensitive they are close to the boundary of the graph perhaps. 

#### Conservation Laws

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(1)
g <- graph_from_edgelist(get_sub_tree(384,bw_hierarchy_edgelist,level=3,n=3),directed = TRUE)
# g <- bw_hierarchy_graph
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),25)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Betweenness Based")

```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(1)
g <- graph_from_edgelist(get_sub_tree(384,level=3,pr_hierarchy_edgelist,n=3))

V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Page-Rank Based")

# install.packages('networkD3')
# library(networkD3)
# pr_d3 <- igraph_to_networkD3(g2)
# dendroNetwork( pr_d3, fontSize = 5, opacity = 0.9)
# 
# install.packages('data.tree')
# library(data.tree)
# library(networkD3)
# 
# g <- get_sub_tree(668,3,pr_hierarchy_edgelist,n=5)
# df <- data.frame(from=vertex_titles[g[,1]],to=vertex_titles[g[,2]])
# root2 <- data.tree::FromDataFrameNetwork(df)
# 
# root2 <- as.list(root2, mode = 'explicit', unname = TRUE)
# 
# diagonalNetwork(List = root2, fontSize = 10, opacity = 0.9, margin = 0)
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(2)
g <- graph_from_edgelist(get_sub_tree(384,level=3,ab_hierarchy_edgelist,n=4 ))
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Attraction Basin")
```

#### Biology Concepts

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(1)
g <- graph_from_edgelist(get_sub_tree(245,bw_hierarchy_edgelist,level=3,n=3),directed = TRUE)
# g <- bw_hierarchy_graph
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),25)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Betweenness Based")
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(3)
g <- graph_from_edgelist(get_sub_tree(245,level=3,pr_hierarchy_edgelist,n=3))

V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Page-Rank Based")
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(3)
g <- graph_from_edgelist(get_sub_tree(245,level=3,ab_hierarchy_edgelist,n=4 ))
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Attraction Basin")
```

#### Quantum Physics

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(1)
g <- graph_from_edgelist(get_sub_tree(2280,bw_hierarchy_edgelist,level=3,n=3),directed = TRUE)
# g <- bw_hierarchy_graph
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Betweenness Based")

```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(4)
g <- graph_from_edgelist(get_sub_tree(2177,level=3,pr_hierarchy_edgelist,n=3))

V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Page-Rank Based")
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
## plotting sub-tree
set.seed(2)
g <- graph_from_edgelist(get_sub_tree(2280,level=3,ab_hierarchy_edgelist,n=3 ))
V(g)$titles <- strtrim(as.character(vertex_titles[V(g)]),30)
iso <- V(g)[igraph::degree(g)==0]
g2 <- delete_vertices(g, iso)
V(g2)$node_label = wrap_strings(V(g2)$titles, 10)
tree_plot_func(g2,2,"Attraction Basin")
```

***

## Conclusion

At the high-level, the results presented above are encouraging in that there are no outrightly nonsensical features visible in them. All of the algorithms are able to identify topics that are similar and close to each other even though the depth and the direction they take are quite different, naturally. Below, we analyse each of the outputs to try and observe patterns in them. 

We note some very interesting structure coming out of each of these algorithms. When looking at the branches rooting out of Conservation Laws, topics such as Kinetic Energy, Work and Energy priciples and articles in Heat and Thermodynamics seem to important branches. While thermodynamics related topics feature in the results of all the three algorithms, Kinetic energy and work principles appear in the traditional centrality based eastimates. The nature of the relationship however is quite different. Betweenness based algorithm places Kinetic energy and work in parallel while page-rank based algorithm places work under topics in Kinetic Energy. The Attraction basin algorithm output is uniquely different in that it branches out into fundamental forces and topics in astro-physics with heat and thermodynamics featuring more prominantly. Overall though, it does feel like these hierarchical relationahips though interesting, aren't particularly accurate. As hypothesised, we would expect a topic such as Conservation laws leading to different topics in Conservation or mechanics such as the law of conservation of energy, momentum, and angular-momentum etc.

We notice a lot of similarities in the structure under Biology Concepts with Energy cycle, Cellular respiration and topics in photosynthesis appearing. The results from the page-rank based algorithm differ the most from the other two with only one of the branches having similar content on the above topics. The position of the article Electron transportation in photosynthesis and cellular respiration seem to be key nodes but their relative positions are quite different in the different outputs.  While there is a hierarchical relationship between these two topics in the betweenness based algorithm, the two nodes are at the same level in the Attraction basin based approach. Fine-tuning the cut-offs to fix such relationships between seemingly important nodes might lead us to a better final output. 

Given my limited knowledge in the field of Quantum Physics, I wouldn't go too deep into analysing the structure for this topic. The hypothesis was that, given a specific and deep topic such as Quantum physics, we would see a lot of similarities in the output of the three algorithms. The hypothesis couldn't have been proved more wrong. Each of the algorithms seem to explore a different aspect within Quantum physics. While the branches seem to go out into pretty advanced topics fairly quickly, you can note that the page-rank based output does have more fundamental topics such "Fundamental Forces", "Wave-particle duality" and "Blackbody Radiation" featuring higher up. 

From analysing these three trees, we do find interesting parts from each of the algorithms. Given the fact that we haven't yet analysed the sensitivity to cut-offs, these must be taken as preliminary results. It is good to observe similarities in the outputs and perhaps the correlation is a strong indicator of the hierarchical relationship. 

Lastly, it must also be noted that for visualization, only a sample of nodes were presented. The absence of certain topics does not imply the absence of the hierarchical relationship in any of the given outputs. 

### The way forward

The next steps in this project are:

1. Exlporing different roots in the full hierarchy graph
2. Quantifying similarities between the algorithms
3. Testing the sensitivity to cutoffs and other parameters
4. Developing a community-detection based iterative approach to extract hierarchy
5. Creating a survey to validate the outputs once tested robustly
6. Applying these methodologies to a much larger network such as the Wikipedia EN Article Network

Some of the challenges that remain are in terms of being able to visualise the full hierarchical graph. I have made some progress in this aspect by exploring D3.js packages for networks (`networkD3`) and hope to be able to present that in some form in the near future. Looking at the full structure would certainly give us a different perspective of the nature and quality of these algorithms. 


