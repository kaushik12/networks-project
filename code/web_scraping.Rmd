---
title: "Scraping"
output: github_document
---

```{r, echo=FALSE, warning=FALSE}
library(rvest)
library(xml2)
library(stringr)
library(stringi)
```

This is the part where we scrape a HTML book to extract hyperlinks and create a network with pages (page names) as nodes and hyperlink between pages as an edge. The webpage used here is HyperPhysics, a Physics and Math HTML textbook developed by Carl R. nave of Georgia State University. We use this because it's comprehensive coverage of most of physics and the linking structure between content pages. It also has a map of the content in some form which could provide validation to the extraction of trees from the link structure.

The url is http://hyperphysics.phy-astr.gsu.edu/hbase/hframe.html

We use the package 'rvest' which is similar to 'beautifulsoup' in Python used for web-scraping. 

```{r}
## We start with the first article in the Index, acceleration as the index page itself is a hframe and we are unable to parse it for links. 

url <- "http://hyperphysics.phy-astr.gsu.edu/hbase/vel2.html#c1"
webpage <- read_html(url)
nodes <- html_nodes(webpage,'a')
titles <- html_text(nodes)
links <- tolower(html_attr(nodes,"href"))
headings <- html_nodes(webpage,'h1')
h_regex <- regexpr(">(.*)<",as.character(headings[1]))
heading <- substr(as.character(headings[1]),(h_regex[1]+1),h_regex[1]+attr(h_regex,"match.length")-2)
heading
print("Links")
links
print("Titles")
titles


attr(regexpr(">(.*)<",as.character(heading)),"match.length")
substr(as.character(heading),20,27)

gsub("<(.*)>(/w)<(.*)>","/w",as.character(heading))

strsplit(strsplit(as.character(heading),">")[[1]][2],"<")[[1]][1]

```

We can see all the links and page titles extracted from the HTML page. Next, we need to traverse through these URLs to find links in those pages. We keep track of the URLs visted and also include some URLs that we do not wish to look into as they do not have topic related content.

```{r}
url_prefix <- "http://hyperphysics.phy-astr.gsu.edu/hbase/"

## Defining a function to get all the hyperlinks in a given webpage
get_links <- function(url,prefix=url_prefix,heading){
  
  full_url <- paste0(prefix,url)
  webpage <- read_html(full_url)
  nodes <- html_nodes(webpage,'a')
  titles <- stri_encode(html_text(nodes), "", "UTF-8")
  links <- html_attr(nodes,"href")
  headings <- html_nodes(webpage,'h1')
  h_regex <- regexpr(">(.*)<",as.character(headings[1]))
  heading <- substr(as.character(headings[1]),(h_regex[1]+1),h_regex[1]+attr(h_regex,"match.length")-2)

  ## Removing any javascript or external (starting with http:// or https://) links 
  titles <- titles[!grepl("^javascript|Javascript|http://|https://|www.|mailto:",links)]
  links <- links[!grepl("^javascript|Javascript|http://|https://|www.|mailto:",links)]
  
  ## Removing multimedia links (.gif/.mp4/.jpg/.png/.mov)
  titles <- titles[!grepl(".gif|.jpg|.png|.mp4|.mov|.jpeg$",links)]
  links <- links[!grepl(".gif|.jpg|.png|.mp4|.mov|.jpeg$",links)]
  
  ## Removing any links which refer to index directory sorting
  titles <- titles[!grepl("[[:punct:]]C[[:punct:]][A-Z][[:punct:]]O[[:punct:]][A-Z]",links)]
  links <- links[!grepl("[[:punct:]]C[[:punct:]][A-Z][[:punct:]]O[[:punct:]][A-Z]",links)]
  
  ## Removing certain URLs to exclude within page references (of the form #xxxD) and removeing extra /
  links <- sub("\\#.*","\\",links)
  links <- gsub("//|///","/",links)
  
  ## Removing the Index and Main page URLs along with any null or missing cases
  excluded_urls <- c("",NA)
  titles <- titles[!links %in% excluded_urls]
  links <- links[!links %in% excluded_urls]
  
  ## If the main page url is a sub-url of the form xx/yy/zz.html, we need to prefix yy/ to results of the form    xx.html
  if(grepl("^.*/",url)){
    string_to_add <- paste0(sub("\\/.*html","\\",url),"/")
    links[!grepl("^../",links)] <- paste0(string_to_add,links[!grepl("^../",links)])
  }
  
  ## Removing any links to sub-domain class as they don;t have physics content
  titles <- titles[!grepl("class/",links)]
  links <- links[!grepl("class/",links)]

  ## Trimming URLs of the form ../xxx.html
  links <- sub("^../","\\",links)
  
  ## Fixing broken URLs 
  links[grepl("^mechanics/vel.html$",links)] <- "vel.html"
  links[grepl("^mechanics/frict.html$",links)] <- "frict.html"
  links[grepl("^forces/particles/quark.html$",links)] <- "particles/quark.html"
  links[grepl("^magnetic/ferro.html$",links)] <- "solids/ferro.html"
  links[grepl("^nuclear/hframe.html$",links)] <- "hframe.html"
  links[grepl("^mechanics/hframe.html$",links)] <- "hframe.html"
  links[grepl("^mechanics/hph.html$",links)] <- "hph.html"
  links[grepl("^thermo/therm/entropcon.html$",links)] <- "thermo/entropcon.html"
  links[grepl("^astro/particles/hadron.html$",links)] <- "particles/hadron.html"
  links[grepl("^astro/grav.html$",links)] <- "grav.html"

  
  ## Removing the Index and Main page URLs along with any null or missing cases
  excluded_urls <- c("hframe.html","hph.html")
  titles <- titles[!links %in% excluded_urls]
  links <- links[!links %in% excluded_urls]
  links <- tolower(links)
  
  unique_links <- trimws(links[!duplicated(links)])
  unique_titles <- trimws(titles[!duplicated(links)]) ## Trimming leading/trailing whitespace
  return(list(main_page=url,main_page_title=heading,page_links=unique_links,page_titles=unique_titles))
}

l <- get_links("thermo//nickel.html",heading="sound")
l

```

Now that we have written a function to extract urls and titles, we will create a data structure for storing this information for all the pages we visit.

```{r}
## Storing the page URL and title in data frame page_details
page_details <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
colnames(page_details) <- c("url","title")

## Initializing a data frame to store the edge list which would be the hyperlinks
edge_list <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
colnames(edge_list) <- c("from_url","to_url")

## Function to add new page details
add_page_details <- function(l,df){
  existing_urls <- df$url
  existing_titles <- df$title
  new_urls <- c(l$main_page,l$page_links)
  new_titles <- c(l$main_page_title,l$page_titles)
  ## Checking if Title is as per page and not the hyperlink text
  if(!(df[df$url == l$main_page,2] == l$main_page_title)){
    df[df$url == l$main_page,2] <- l$main_page_title
  }
  ## Removing duplicates before appending
  new_titles <- new_titles[!new_urls %in% existing_urls]
  new_urls <- new_urls[!new_urls %in% existing_urls]
  temp_df <- data.frame(url=new_urls,title=new_titles)
  return(rbind(df,temp_df))
}

## Function to add new edge information
get_edges <- function(l){
  if(length(l$page_links) > 0){
    return(data.frame(from_url=l$main_page,to_url=l$page_links))
  }else{
    return(NULL)
  }
}

# page_details <- add_page_details(l,df = page_details )
# edge_list <- rbind(edge_list,get_edges(l))
```

```{r, warning=FALSE,echo=FALSE}
## Creating a list to store all the URLs vsiited
visited_urls <- NA
error_urls <- NA
errors <- NA
unvisited_urls <- c("acca.html")
page_details <- rbind(page_details,c("acca.html","Acceleration"))
page_details <- page_details[-1,]

# 133300, 4:55pm
counter <- 0
st <- proc.time()
while(length(unvisited_urls) > 0){
  tryCatch(
    {
      l <-get_links(unvisited_urls[1],heading=page_details$title[which(page_details$url == unvisited_urls[1])])
      page_details <- add_page_details(l,df = page_details )
      edge_list <- rbind(edge_list,get_edges(l))
    },
    error=function(cond){
      # print(unvisited_urls[1])
      error_urls <<- c(error_urls,unvisited_urls[1])
      # errors <<- c(errors,cond)
    },
    finally={
      visited_urls <- c(visited_urls,unvisited_urls[1])
      unvisited_urls <- page_details$url[!page_details$url %in% visited_urls]
      counter <- counter + 1   
      # print(length(unvisited_urls))
    }
  )
}
print(counter)
error_urls <- error_urls[!is.na(error_urls)]
proc.time() - st
```

Small script to check broken URLs

```{r}
# Check
to_check = "isq.html"
edge_list$from_url[edge_list$to_url == to_check][2]
page_details[which(page_details$url == to_check),]

# Fix to continue
page_details$url[which(page_details$url == unvisited_urls[1])] <- "grav.html"
unvisited_urls[1] <- "grav.html"

```


Script to fix broken URLs. I find that most broken URLs are of the form xxx/yyy/zzz.html where the correct URL is supposed to be yyy/zzz.html
```{r}
## Removing Error URLs within class/.. and kinetic and quantum index directories

all_error_urls <- error_urls

error_urls <- error_urls[!(grepl("^class/",error_urls))]
error_urls <- error_urls[!(grepl("^kinetic/imgkin",error_urls))]
error_urls <- error_urls[!(grepl("^kinetic/kinpic",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/imgqua",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/modpic",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/imgqua/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^quantum/modpic/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^kinetic/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^Kinetic/[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^Kinetic/imgkin//[[:punct:]]",error_urls))]
error_urls <- error_urls[!(grepl("^Kinetic/kinpic//[[:punct:]]",error_urls))]
length(error_urls)
```



```{r, warning=FALSE,echo=FALSE}

error_urls <- gsub("//|///","/",error_urls)
fixed_error_urls <- data.frame(matrix(ncol=2),stringsAsFactors = FALSE)
unvisited_urls <- ""
st <- proc.time()
st
for(i in seq_along(error_urls)){
  fixed_error_urls[i,1] <- error_urls[i]
  tryCatch(
    {
      fixed_error_urls[i,2] <- substring(error_urls[i],regexpr("/",error_urls[i])[1]+1)
      temp_url <- paste0(url_prefix,fixed_error_urls[i,2])
      if(!(fixed_error_urls[i,2] %in% edge_list$from_url)){
        temp_page <- read_html(temp_url)  
      }
      ## Replace URL in Page Details
      page_details$url[which(page_details$url %in% error_urls[i])] <- fixed_error_urls[i,2]
      ## Replace URL in Edge Details
      edge_list$from_url[which(edge_list$from_url %in% error_urls[i])] <- fixed_error_urls[i,2]
      edge_list$to_url[which(edge_list$to_url %in% error_urls[i])] <- fixed_error_urls[i,2]
      ## Add to Unvisited URLs if not visited
      if(!fixed_error_urls[i,2] %in% edge_list$from_url){
        unvisited_urls <- c(unvisited_urls,fixed_error_urls[i,2])
      }
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    },finally = {
      if(i%%100 == 0){
        print(i)
      }
    }
  )
}
proc.time() - st

error_url_fixes <- fixed_error_urls[!is.na(fixed_error_urls$X2),]
for(i in seq_along(error_url_fixes)){
  err_url <- error_url_fixes$X1[i]
  fixed_url <- error_url_fixes$X2[i]
  page_details$url[which(page_details$url == err_url)] <- fixed_url
  edge_list$from_url[which(edge_list$from_url == err_url)] <- fixed_url  
  edge_list$to_url[which(edge_list$to_url == err_url)] <- fixed_url  
  if(!fixed_url %in% edge_list$from_url){
        unvisited_urls <- c(unvisited_urls,fixed_url)
      }
}
fixed_error_urls <- fixed_error_urls[is.na(fixed_error_urls$X2),]

## Check if these URls are infact broken (some seem to work on manual inspection)
for(i in seq_along(fixed_error_urls$X1)){
  tryCatch(
    {
      temp_url <- paste0(url_prefix,fixed_error_urls[i,1])
      temp_page <- read_html(temp_url)   
      fixed_error_urls[i,2] <- fixed_error_urls[i,1]
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    }
  )
}

# ## First fixes (sound urls without "/Sound")
# fixed_error_urls$X2[104:length(fixed_error_urls$X2)] <- paste0("Sound/",fixed_error_urls$X1[104:length(fixed_error_urls$X2)])
# ## Re-run above loop

# ## Second fixes (Sound URLs which work but under error URLs)                                                  
# fixed_error_urls$X2[81:length(fixed_error_urls$X2)] <- fixed_error_urls$X1[81:length(fixed_error_urls$X2)]
# ## Re-run above loop

## removing spaces (\r) in the url which was causing an error
fixed_error_urls$X2[grep("\r",fixed_error_urls$X1)] <- gsub("\r","",fixed_error_urls[grep("\r",fixed_error_urls$X1),1])
  

## Fixing organic/AMIDE.HTML URLs
fixed_error_urls$X2[grep("/AMIDE.HTML",fixed_error_urls$X1)] <- gsub("/AMIDE.HTML","",fixed_error_urls[grep("/AMIDE.HTML",fixed_error_urls$X1),1])



## Adding parent domain to URL to check if it works
for(i in seq_along(fixed_error_urls$X1)){
  tryCatch(
    {  
      parent_url <- edge_list$from_url[edge_list$to_url == fixed_error_urls[i,1]][2]
      parent_domain <- strsplit(parent_url,"/")[[1]][max(1,length(strsplit(parent_url,"/")[[1]])-1)]
      fixed_url <- paste0(parent_domain,"/",fixed_error_urls$X1[i])
      temp_url <- paste0(url_prefix,fixed_url)
      temp_page <- read_html(temp_url)   
      fixed_error_urls[i,2] <- fixed_url
    },
    error=function(cond){
      fixed_error_urls[i,2] <<- NA
    }
  )
}


fixed_error_urls$X2[grep("relcon.html",fixed_error_urls$X1)] <-"relativ/relcon.html"
fixed_error_urls$X2[grep("reldop3.html",fixed_error_urls$X1)] <-"relativ/reldop3.html"
fixed_error_urls$X2[grep("music/..sound/timbre.html",fixed_error_urls$X1)] <-"sound/timbre.html"


## Re-run above loop
error_url_fixes <- fixed_error_urls[!is.na(fixed_error_urls$X2),]
for(i in seq_along(error_url_fixes)){
  err_url <- error_url_fixes$X1[i]
  fixed_url <- error_url_fixes$X2[i]
  page_details$url[which(page_details$url == err_url)] <- fixed_url
  edge_list$from_url[which(edge_list$from_url == err_url)] <- fixed_url  
  edge_list$to_url[which(edge_list$to_url == err_url)] <- fixed_url  
  if(!fixed_url %in% edge_list$from_url){
        unvisited_urls <- c(unvisited_urls,fixed_url)
      }
}
fixed_error_urls <- fixed_error_urls[is.na(fixed_error_urls$X2),]


## Ignoring 27 broken URLs for now

```

```{r}
## Remove URLs with reference to class sub-domain as they don't pertain to physics content
page_details <- page_details[!grepl("class/",page_details$url),]
edge_list <- edge_list[!grepl("class/",edge_list$from_url),]
edge_list <- edge_list[!grepl("class/",edge_list$to_url),]
page_details$url <- gsub("//|///","/",page_details$url) 
edge_list$from_url <- gsub("//|///","/",edge_list$from_url) 
edge_list$to_url <- gsub("//|///","/",edge_list$to_url) 

## Removing self edges from edge_list and 1st row with an NA
edge_list <- edge_list[-1,]
edge_list <- edge_list[-which(edge_list$from_url == edge_list$to_url),]

## Checking for and removing duplicate entries in page_details and edge_list
edge_list <- edge_list[!duplicated(edge_list), ]
page_details <- page_details[!duplicated(page_details),] ## URL and Title match
page_details <- page_details[!duplicated(page_details$url),] ## Only URL duplicated
sum(duplicated(page_details$title))
duplicated_pages <- page_details[duplicated(page_details$title),]  
duplicated_pages <- duplicated_pages[order(duplicated_pages[,2]),]

merge_url_duplicates <- function(df){
  
}

page_details[duplicated(page_details$title),]
```



```{r}
setwd("./Data")
write.csv(page_details,file="page_details.csv",row.names = FALSE)
write.csv(edge_list,file="edge_list.csv",row.names = FALSE)
```

Testing the algo:
1. start from a specific topic
2. snowball search (target say 300 and induced subgraph)
3. start at this higher level 
4. compare the resutls for both



